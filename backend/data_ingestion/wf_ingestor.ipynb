{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fcf11ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'logger' from 'logger' (c:\\Users\\habou\\OneDrive\\Bureau\\python-projects\\renaulution-rag\\backend\\data_ingestion\\logger.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdf_ingestor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_and_split_pdf\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mytb_ingestor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_and_split_ytb\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvectorestore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m index_documents, create_vectorstore\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\habou\\OneDrive\\Bureau\\python-projects\\renaulution-rag\\backend\\data_ingestion\\vectorestore.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'logger' from 'logger' (c:\\Users\\habou\\OneDrive\\Bureau\\python-projects\\renaulution-rag\\backend\\data_ingestion\\logger.py)"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from tqdm import tqdm\n",
    "from pdf_ingestor import extract_and_split_pdf\n",
    "from ytb_ingestor import extract_and_split_ytb\n",
    "from vectorestore import index_documents, create_vectorstore\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(embedding_model, index_path):\n",
    "    try:\n",
    "        dim = len(embedding_model.embed_query(\"test\"))\n",
    "        logger.info(f\"Création d'un index FAISS vide (dim = {dim})\")\n",
    "\n",
    "        dummy_doc = Document(page_content=\"placeholder\", metadata={})\n",
    "        vectorstore = FAISS.from_documents(\n",
    "            documents=[dummy_doc],\n",
    "            embedding=embedding_model,\n",
    "            normalize_L2=True\n",
    "        )\n",
    "        vectorstore.index.reset()  \n",
    "        vectorstore.docstore._dict.clear()  \n",
    "        vectorstore.index_to_docstore_id.clear() \n",
    "\n",
    "\n",
    "        logger.info(f\"Sauvegarde de l'index vide dans: {index_path}\")\n",
    "        vectorstore.save_local(index_path)\n",
    "\n",
    "        logger.info(\"Index vide (cosinus) créé avec succès.\")\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Erreur lors de la création de l’index vide: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def index_documents(chunks, embedding_model, index_storing_path, batch_size=64):\n",
    "    vectorstore = FAISS.load_local(index_storing_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "    try:\n",
    "        logger.info(f\"Début de l’indexation de {len(chunks)} chunks en batches de {batch_size}...\")\n",
    "\n",
    "        logger.info(\"Calcul des embeddings et indexation par batch avec barre de progression...\")\n",
    "        for i in tqdm(range(0, len(chunks), batch_size), desc=\"Indexing\", unit=\"batch\"):\n",
    "            batch = chunks[i:i + batch_size]\n",
    "\n",
    "            logger.info(f\"Indexation du batch {i//batch_size + 1}...\")\n",
    "            vectorstore.add_documents(batch)\n",
    "\n",
    "        logger.info(f\"Sauvegarde de l’index local dans : {index_storing_path}\")\n",
    "        vectorstore.save_local(index_storing_path)\n",
    "        logger.info(\"Indexation terminée avec succès.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Erreur lors de l’indexation : {e}\", exc_info=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5204866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\habou\\AppData\\Local\\Temp\\ipykernel_660\\2590460214.py:1: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceBgeEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"../../multilingual-e5-large\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    "    query_instruction=\"query: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3eb3d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(embedding_model, index_path):\n",
    "    try:\n",
    "        dim = len(embedding_model.embed_query(\"test\"))\n",
    "        logger.info(f\"Création d'un index FAISS vide (dim = {dim})\")\n",
    "\n",
    "        dummy_doc = Document(page_content=\"placeholder\", metadata={})\n",
    "        vectorstore = FAISS.from_documents(\n",
    "            documents=[dummy_doc],\n",
    "            embedding=embedding_model,\n",
    "            normalize_L2=True\n",
    "        )\n",
    "        vectorstore.index.reset()  \n",
    "        vectorstore.docstore._dict.clear()  \n",
    "        vectorstore.index_to_docstore_id.clear() \n",
    "\n",
    "\n",
    "        logger.info(f\"Sauvegarde de l'index vide dans: {index_path}\")\n",
    "        vectorstore.save_local(index_path)\n",
    "\n",
    "        logger.info(\"Index vide (cosinus) créé avec succès.\")\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Erreur lors de la création de l’index vide: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def index_documents(chunks, embedding_model, index_storing_path, batch_size=64):\n",
    "    vectorstore = FAISS.load_local(index_storing_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "    try:\n",
    "        logger.info(f\"Début de l’indexation de {len(chunks)} chunks en batches de {batch_size}...\")\n",
    "\n",
    "        logger.info(\"Calcul des embeddings et indexation par batch avec barre de progression...\")\n",
    "        for i in tqdm(range(0, len(chunks), batch_size), desc=\"Indexing\", unit=\"batch\"):\n",
    "            batch = chunks[i:i + batch_size]\n",
    "\n",
    "            logger.info(f\"Indexation du batch {i//batch_size + 1}...\")\n",
    "            vectorstore.add_documents(batch)\n",
    "\n",
    "        logger.info(f\"Sauvegarde de l’index local dans : {index_storing_path}\")\n",
    "        vectorstore.save_local(index_storing_path)\n",
    "        logger.info(\"Indexation terminée avec succès.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Erreur lors de l’indexation : {e}\", exc_info=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1ee4d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:53:26] INFO - Création d'un index FAISS vide (dim = 1024)\n",
      "[12:53:26] INFO - Sauvegarde de l'index vide dans: index_path\n",
      "[12:53:26] INFO - Index vide (cosinus) créé avec succès.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x11d240b3980>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_vectorstore(embedding_model, \"index_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90b60834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': '25be2375-56d2-46a2-b2bd-5aa27f401a7f'}, page_content='Haboubacar Tidjani Boukari Data Scientist (NLP - GenAI - LLM) Sp´ecialis´e en NLP, IA g´en´erative et machine learning, avec une expertise av´er´ee dans le d´eveloppement d’applications IA et l’optimisation de mod`eles. Fort de plusieurs exp´eriences r´eussies en NLP, MLOps, moteurs de recherche, et en gestion de projets. Je nourris depuis tout jeune l’envie et le rˆeve de travailler un jour chez Total Energie.'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': '03b64f32-6ad6-4e53-ad59-2867412688b0'}, page_content='Langages : Python, Node.js, SQL NLP & IA Gen : LangChain, LangGraph, Chainlit, FAISS, ONNX Runtime, Ollama, RAG Unsloth, LLM\\nBig Data & Cloud: Spark, Hive, MySQL, PostgreSQL, Redis, Cloud Azure, GCP\\nD´eploiement : Git, Docker, FastAPI, Flask Outils analytiques : Databricks, Power BI, MLFLOW, Alteryx Soft Skills : Leadership, gestion de projet agile, proactivit´e'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': '874a34a5-44d0-484c-8b45-87566821e79d'}, page_content='Mc2i Consultant confirm´e - Data & IA Depuis Oct 2022 Mission : Caisse des D´epˆots (Depuis mars 2024) – D´eveloppement d’un algorithme IA pour identifier des certifications en lien avec la transition ´ecologique, avec une pr´ecision de classification de 80% (Embedding CamemBERT + Cosine Similarity & Fine-tuning CamemBERT)\\n– Clustering s´emantique de 25000 certifications pour l’indexation documentaire (Cosine Similarity + algorithme'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': 'a22974b3-552f-4058-98d6-956b1d9266f9'}, page_content='Find-Union) – D´eveloppement d’un moteur s´emantique de recherche de fichiers sur le One-drive du service data (Langchain, Faiss,\\nCamembert, Streamlit) – D´eploiement d’une API de vectosation de documents (Hugging Face, ONNX, FastAPI)\\n– Conception et optimisation des pipelines de donn´ees via Alteryx et Pyspark : fourniture automatique de donn´ees en'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': '86ea9287-a6c5-48b5-9e90-6a197d6c6719'}, page_content='r´eponse aux demandes m´etiers – R´ef´erent technique OpenData : cr´eation, publication et maintenance des pipelines d’alimentation des jeux de donn´ees\\n– Conduite d’ateliers de restitution des projets aux ´equipes m´etiers\\nMission : ˆIle-De-France-Mobilit´es (Oct 2022 - Mars 2024) – Receuil de besoin m´etier, cadrage, r´eadaction de specs fonctionnelles et macro chiffrage'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': 'd27d8f25-bcc3-4405-ad9f-5b5f89249457'}, page_content='– D´eploiement d’un RAG sur les sp´ecifications fonctionnelles & techniques de l’´equipe IVTR\\n– Conception et d´eploiement d’un Agent IA (interfac´e avec GPT-4) afin de simuler des donn´ees de transport pour tests\\n– Conception d’une interface de Chat (Flask + JQuery) de g´en´eration automatis´ee de flux de donn´ees & mise en place'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': 'dae638fd-0ab5-4330-8eef-addbcf84c7b7'}, page_content='d’une CI/CD – Automatisation des tests de non-r´egression, r´eduisant le temps d’ex´ecution manuelle de 5 jours `a 3 heures, soit un gain\\nde productivit´e de 90% – Conduite des tests de non-r´egression de l’application de recherche d’itin´eraire d’IDFM, optimis´ee pour les JO Paris 2024\\n– R´edaction des sp´ecifications, gestion des anomalies, et suivi technique du relais IVTR'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': 'd0358b6f-7ff5-4ac9-b25e-fd2f6413e599'}, page_content='– Animation des instances agiles et coordination des tests de connexion de nouveaux partenaires\\nStack technique: Python, Node.js, SQL, Pyspark, Flask, langchain, ElasticSearch, Kibana, GPT-4, Azure DevOps, App Sevices,\\nDatabricks, Alteryx, Docker, Postman PwC France Data Scientist Sept 2021 – Aoˆut 2022 1/2 – Entrainement et d´eploiement en production de huits mod`eles de classification de mails sur Databricks via MLFLOW'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': '11643dfc-73c2-49d9-b066-1b99674c6e81'}, page_content='(mise en concurrence des mod`eles CamemBERT, SVM et Perceptron)\\n– Conception et mise en œuvre d’une pipeline MLOps robuste, int´egrant une boucle r´etroactive pour l’´evaluation continue\\net la d´etection du data drift – Cr´eation d’un robot sous UiPath interfac¸ant le mod`ele de classification de mails et la boucle de r´etroaction, automatisant\\nla classification des mails dans la boˆıte de r´eception client'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': 'ea0f478f-4c90-4d3e-8ed3-bd6bb429acf4'}, page_content='la classification des mails dans la boˆıte de r´eception client\\n– D´eveloppement d’un syst`eme de recommandation d’archivage performant permettant d’identifier et d’´eliminer les\\ndocuments obsol`etes, entraˆınant une r´eduction de 20% de l’espace de stockage utilis´e\\n– Collaboration avec les ´equipes de d´eveloppement et gestionnaires de projets pour l’alignement des solutions avec les\\nbesoins clients.'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': 'ddfaa512-d2cb-4e06-a6c1-36b7d4ebc241'}, page_content='besoins clients.\\nStack technique: Python, Keras, BERT, Scikit-learn, UiPath, Power BI, Databricks,MLflow\\nINRAE Data Scientist - Deep Learning Mai 2021 – Aoˆut 2021 – Cr´eation d’un mod`ele Keras de classification d’images collect´ees par drone pour la d´etection et la cartographie des dysfonctionnements du syst`eme d’irrigation dans un verger de pommiers, permettant de reconnaˆıtre les arbres en situation de d´eficit ou d’exc`es hydrique.'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': 'cd136678-54aa-48af-91a8-d0fe889df9c5'}, page_content='Principales tˆaches r´ealis´ees :\\n– Acquisition des images par vols de drone sur une parcelle exp´erimentale de pommiers.\\n– Pr´etraitement et ajustement des images pour une meilleure qualit´e d’analyse.\\n– Augmentation des donn´ees pour am´eliorer la robustesse du mod`ele.\\n– Entraˆınement et validation du mod`ele de classification d’images.\\n– D´eveloppement d’une API Flask et exposition de l’endpoint `a une web app consommant le mod`ele.'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': '89b0bafe-cf0d-4f9c-a26f-7891ed06db9d'}, page_content='Stack technique: Python, Keras, Flask Geographiclib, Linux, SSH\\nSoutien scolaire en maths : niveau lyc´ee Dec 2019 - Avr 2021 – S´election et pr´eparation d’exercices adapt´es, Aide aux devoirs, Suivi et bilan de progression r´eguliers avec les parents\\nCo-Auteur de TSM-ANNALEX Depuis mars 2019 – Production de l’annale TSM-ANNALEX, recueil des ´epreuves de BAC maths, destin´e aux terminales C du Niger'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': '59c1fcf7-1414-4039-b010-f4fb41624900'}, page_content='– Mise en place d’une ´equipe de commercialisation sur l’´etendue du pays\\nEISTI - Cergy Diplˆome d’ing´enieur en Intelligence Artificielle\\n2019-2022 Lyc´ee Moulay Idriss - Maroc Classes Pr´eparatoires aux Grandes Ecoles (MPSI-MP)\\n2017-2019 Lyc´ee d’Excellence - Niger Baccalaur´eat Sciences Maths 2014-2017 Prix et distinction Sept 2017 – R´ecipiendaire du prix d’excellence pour meilleur bachelier national, session 2017 au Niger'),\n",
       " Document(metadata={'source': 'CV_Data_Science_Haboubacar_TB.pdf', 'id': '8b6b5c23-e127-457b-99e0-5a953a08792f'}, page_content='Projets Perso IA – Automatisation de ma veille technologique sur notion (Notion API, GPT-4o, GCP) : notion auto educ\\n– Chatbot sur mon parcours scolaire et porfessionnel (Langchain, FAISS, Flask, RAG, GPT-4o, GCP) : know me by my AI\\n– Plateforme de d´ebat entre deux IA sur un sujet donn´e (GPT-4o, Claude-3, Langchain, Streamlit) : debat gpt vs claude\\nCertifications – Agile PSPO I (Juillet 2023) 2/2')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"../../data/CV_Data_Science_Haboubacar_TB.pdf\"\n",
    "pdf_chunks = extract_and_split_pdf(file_path, 450, 64)\n",
    "pdf_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f34e47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_and_split_ytb(\"https://www.youtube.com/watch?v=VfIeaIFSCQA\", 450, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "698421ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:53:33] INFO - Début de l’indexation de 15 chunks en batches de 4...\n",
      "[12:53:33] INFO - Calcul des embeddings et indexation par batch avec barre de progression...\n",
      "Indexing:   0%|          | 0/4 [00:00<?, ?batch/s][12:53:33] INFO - Indexation du batch 1...\n",
      "Indexing:  25%|██▌       | 1/4 [00:01<00:03,  1.24s/batch][12:53:34] INFO - Indexation du batch 2...\n",
      "Indexing:  50%|█████     | 2/4 [00:02<00:02,  1.19s/batch][12:53:35] INFO - Indexation du batch 3...\n",
      "Indexing:  75%|███████▌  | 3/4 [00:03<00:01,  1.23s/batch][12:53:36] INFO - Indexation du batch 4...\n",
      "Indexing: 100%|██████████| 4/4 [00:04<00:00,  1.16s/batch]\n",
      "[12:53:37] INFO - Sauvegarde de l’index local dans : index_path\n",
      "[12:53:37] INFO - Indexation terminée avec succès.\n"
     ]
    }
   ],
   "source": [
    "index_documents(pdf_chunks, embedding_model, \"index_path\", batch_size=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
