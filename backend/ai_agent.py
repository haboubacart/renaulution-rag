import os
from langchain.agents import initialize_agent, Tool, AgentType
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
import warnings
warnings.filterwarnings("ignore")


# Initialisation du LLM
"""llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",  
    temperature=0.7
)"""

#Initialiser le LLM (gpt-4o par dÃ©faut)
def load_llm():
    return ChatOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        model="gpt-4o",
        temperature=0.1
    )

# ğŸ§  Prompt personnalisÃ© pour le RAG
def build_rag_prompt():
    prompt_template = """
    Utilise uniquement les Ã©lÃ©ments de contexte suivants pour rÃ©pondre Ã  la question.
    Si vous ne connaissez pas la rÃ©ponse, dites simplement que vous ne savez pas.

    Contexte: {context}

    Question: {question}

    RÃ©ponse utile:
    """
    return PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# ğŸ” CrÃ©e une chaÃ®ne RAG basÃ©e sur un retriever
def create_rag_chain(llm, retriever):
    prompt = build_rag_prompt()
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )

# ğŸ› ï¸ Fonction d'accÃ¨s pour le RAG tool
def retriever_relevant_docs_factory(rag_chain):
    def _retriever(query: str):
        result = rag_chain.invoke({"query": query})
        return result["result"]
    return _retriever

# ğŸš€ Construire l'agent LangChain
def build_agent(retriever):
    llm = load_llm()
    rag_chain = create_rag_chain(llm, retriever)
    search_func = retriever_relevant_docs_factory(rag_chain)

    rag_tool = Tool(
        name="rag",
        func=search_func,
        description="Recherche dans les documents de Renault fournis en contexte pour rÃ©pondre Ã  des questions."
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )

    agent = initialize_agent(
        tools=[rag_tool],
        llm=llm,
        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
        memory=memory,
        verbose=False,
        handle_parsing_errors=True
    )

    return agent

# ğŸ¤ Fonction d'appel de l'agent
def ask_agent(agent, question: str) -> str:
    return agent.run(question)

